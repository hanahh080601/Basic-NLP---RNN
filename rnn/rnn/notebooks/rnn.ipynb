{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bap/hana/Basic-NLP-RNN/rnn/rnn\n"
     ]
    }
   ],
   "source": [
    "%cd /home/bap/hana/Basic-NLP-RNN/rnn/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    '''\n",
    "    Config class defines dataset path and hyperparameters.\n",
    "    '''\n",
    "    data_train_url = 'dataset/shakespeare_train.txt'\n",
    "    data_val_url = 'dataset/shakespeare_valid.txt'\n",
    "    n_hidden = 512\n",
    "    n_layers = 2\n",
    "    epochs = 25 \n",
    "    n_seqs = 128\n",
    "    n_steps = 100\n",
    "    lr = 0.001\n",
    "    clip = 5\n",
    "    cuda = False\n",
    "    dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    '''\n",
    "    Load data from data path, preprocess (tokenize & one-hot encode) and get data in array type.\n",
    "    '''\n",
    "    def __init__(self, data_train_url = Config.data_train_url, data_val_url = Config.data_val_url):\n",
    "        with io.open (data_train_url, 'r') as f:\n",
    "            self.text_train = f.read()\n",
    "        with io.open (data_val_url, 'r') as f:\n",
    "            self.text_val = f.read()\n",
    "\n",
    "    def char_tokenize(self):\n",
    "        self.chars = tuple(set(self.text_train))\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        self.train_data = np.array([self.char2int[ch] for ch in self.text_train])\n",
    "        self.val_data = np.array([self.char2int[ch] for ch in self.text_val])\n",
    "\n",
    "    def one_hot_encode(self, arr, n_labels):\n",
    "        one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "        one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "        one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "        return one_hot\n",
    "\n",
    "    def get_data(self):\n",
    "        self.char_tokenize()\n",
    "        return self.train_data, self.val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded chars in train: [49 51 41 14  3 25 64 51  3 51  5 57 53 37 44  4 57 66 29 41 57 25 33 57\n",
      " 25  6 41 29 21 57 57 42 25 36 53 31 25 66 11 41  3  7 57 41 20 25  7 57\n",
      " 36 41 25 13 57 25 14  6 57 36 10 38 44 44 22 23 23 37 44 30  6 57 36 10\n",
      " 20 25 14  6 57 36 10 38 44 44 49 51 41 14  3 25 64 51  3 51  5 57 53 37\n",
      " 44 28 29 11]\n",
      "Number of chars in vocab:  67\n",
      "Train text:  First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "data = Dataset()\n",
    "train_data, val_data = data.get_data()\n",
    "print(\"Encoded chars in train:\", train_data[:100])\n",
    "print(\"Number of chars in vocab: \", len(data.chars))\n",
    "print(\"Train text: \", data.text_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    '''\n",
    "    Load data from dataset in batches (batches = n_seqs * n_steps)\n",
    "    '''\n",
    "    def __init__(self, train, val):\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, arr, n_seqs, n_steps):\n",
    "        '''\n",
    "        Create a generator that returns batches of size\n",
    "        n_seqs x n_steps from arr.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        arr: np.array\n",
    "            Array you want to make batches from\n",
    "        n_seqs: int\n",
    "            Batch size, the number of sequences per batch\n",
    "        n_steps: int\n",
    "            Number of sequence steps per batch\n",
    "        '''\n",
    "        batch_size = n_seqs * n_steps\n",
    "        n_batches = len(arr) // batch_size\n",
    "        arr = arr[:n_batches * batch_size]\n",
    "        arr = arr.reshape((n_seqs, -1))\n",
    "        \n",
    "        for n in range(0, arr.shape[1], n_steps):\n",
    "            x = arr[:, n: n + n_steps]\n",
    "            y = np.zeros_like(x)\n",
    "            try:\n",
    "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
    "            except IndexError:\n",
    "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[49, 51, 41, 14,  3]]), array([[51, 41, 14,  3, 25]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = DataLoader(train_data, val_data)\n",
    "next(data_loader(train_data, 1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_steps=Config.n_steps, n_hidden=Config.n_hidden, n_layers=Config.n_layers,\n",
    "                    drop_prob=Config.dropout, lr=Config.lr):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr      \n",
    "        self.lstm = nn.LSTM(vocab_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)        \n",
    "        self.dropout = nn.Dropout(drop_prob)      \n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' \n",
    "        Initialize weights for fully connected layer \n",
    "        '''\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' \n",
    "        Initializes hidden state \n",
    "        '''\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        ''' \n",
    "        Forward pass through the network. \n",
    "        These inputs are x, and the hidden/cell state `hc`. \n",
    "        '''\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = x.reshape(x.size()[0] * x.size()[1], self.n_hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data, val_data, epochs=Config.epochs, n_seqs=Config.n_seqs, \n",
    "          n_steps=Config.n_steps, lr=Config.lr, clip=Config.clip, cuda=Config.cuda):\n",
    "    ''' \n",
    "        Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ----------------\n",
    "        net: RNN network\n",
    "        train_data: text data to train the network\n",
    "        val_data: text data to validate the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        cuda: Train with CUDA on a GPU\n",
    "    '''\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in data_loader(train_data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = data.one_hot_encode(x, net.vocab_size)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % 10 == 0:\n",
    "                \n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in data_loader(val_data, n_seqs, n_steps):\n",
    "                    x = data.one_hot_encode(x, net.vocab_size)\n",
    "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(67, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=67, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "net = RNN(vocab_size=len(data.chars))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.4232... Val Loss: 3.4073\n",
      "Epoch: 1/1... Step: 20... Loss: 3.2957... Val Loss: 3.2593\n",
      "Epoch: 1/1... Step: 30... Loss: 3.1133... Val Loss: 3.0987\n",
      "Epoch: 1/1... Step: 40... Loss: 2.9358... Val Loss: 2.9181\n",
      "Epoch: 1/1... Step: 50... Loss: 2.8185... Val Loss: 2.7483\n",
      "Epoch: 1/1... Step: 60... Loss: 2.6611... Val Loss: 2.6403\n",
      "Epoch: 1/1... Step: 70... Loss: 2.5618... Val Loss: 2.5690\n",
      "Epoch: 1/1... Step: 80... Loss: 2.5486... Val Loss: 2.5128\n",
      "Epoch: 1/1... Step: 90... Loss: 2.4703... Val Loss: 2.4704\n",
      "Epoch: 1/1... Step: 100... Loss: 2.4520... Val Loss: 2.4362\n",
      "Epoch: 1/1... Step: 110... Loss: 2.3872... Val Loss: 2.4088\n",
      "Epoch: 1/1... Step: 120... Loss: 2.3563... Val Loss: 2.3763\n",
      "Epoch: 1/1... Step: 130... Loss: 2.3285... Val Loss: 2.3553\n",
      "Epoch: 1/1... Step: 140... Loss: 2.3048... Val Loss: 2.3438\n",
      "Epoch: 1/1... Step: 150... Loss: 2.2897... Val Loss: 2.3164\n",
      "Epoch: 1/1... Step: 160... Loss: 2.2828... Val Loss: 2.2957\n",
      "Epoch: 1/1... Step: 170... Loss: 2.2815... Val Loss: 2.2785\n",
      "Epoch: 1/1... Step: 180... Loss: 2.2250... Val Loss: 2.2624\n",
      "Epoch: 1/1... Step: 190... Loss: 2.2200... Val Loss: 2.2440\n",
      "Epoch: 1/1... Step: 200... Loss: 2.2084... Val Loss: 2.2368\n",
      "Epoch: 1/1... Step: 210... Loss: 2.2025... Val Loss: 2.2179\n",
      "Epoch: 1/1... Step: 220... Loss: 2.2143... Val Loss: 2.2082\n",
      "Epoch: 1/1... Step: 230... Loss: 2.1506... Val Loss: 2.1881\n",
      "Epoch: 1/1... Step: 240... Loss: 2.1580... Val Loss: 2.1780\n",
      "Epoch: 1/1... Step: 250... Loss: 2.1095... Val Loss: 2.1681\n",
      "Epoch: 1/1... Step: 260... Loss: 2.1127... Val Loss: 2.1583\n",
      "Epoch: 1/1... Step: 270... Loss: 2.0801... Val Loss: 2.1466\n",
      "Epoch: 1/1... Step: 280... Loss: 2.1025... Val Loss: 2.1315\n",
      "Epoch: 1/1... Step: 290... Loss: 2.0596... Val Loss: 2.1235\n",
      "Epoch: 1/1... Step: 300... Loss: 2.1095... Val Loss: 2.1113\n",
      "Epoch: 1/1... Step: 310... Loss: 2.0679... Val Loss: 2.1006\n",
      "Epoch: 1/1... Step: 320... Loss: 2.0659... Val Loss: 2.0874\n",
      "Epoch: 1/1... Step: 330... Loss: 2.0315... Val Loss: 2.0801\n"
     ]
    }
   ],
   "source": [
    "train(net=net, train_data=train_data, val_data=val_data, epochs=1, n_seqs=128, n_steps=100, lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6996669039b24f13aef7be20d9606477de00a70217801a8b6318938f205f3171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
