{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bap/hana/Basic-NLP-RNN/rnn/rnn\n"
     ]
    }
   ],
   "source": [
    "%cd /home/bap/hana/Basic-NLP-RNN/rnn/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    '''\n",
    "    Config class defines dataset path and hyperparameters.\n",
    "    '''\n",
    "    data_train_url = 'dataset/shakespeare_train.txt'\n",
    "    data_val_url = 'dataset/shakespeare_valid.txt'\n",
    "    n_hidden = 512\n",
    "    n_layers = 2\n",
    "    epochs = 25 \n",
    "    n_seqs = 128\n",
    "    n_steps = 100\n",
    "    lr = 0.001\n",
    "    clip = 5\n",
    "    cuda = False\n",
    "    dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    '''\n",
    "    Load data from data path, preprocess (tokenize & one-hot encode) and get data in array type.\n",
    "    '''\n",
    "    def __init__(self, data_train_url = Config.data_train_url, data_val_url = Config.data_val_url):\n",
    "        with io.open (data_train_url, 'r') as f:\n",
    "            self.text_train = f.read()\n",
    "        with io.open (data_val_url, 'r') as f:\n",
    "            self.text_val = f.read()\n",
    "\n",
    "    def char_tokenize(self):\n",
    "        self.chars = tuple(set(self.text_train))\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        self.train_data = np.array([self.char2int[ch] for ch in self.text_train])\n",
    "        self.val_data = np.array([self.char2int[ch] for ch in self.text_val])\n",
    "\n",
    "    def one_hot_encode(self, arr, n_labels):\n",
    "        one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "        one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "        one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "        return one_hot\n",
    "\n",
    "    def get_data(self):\n",
    "        self.char_tokenize()\n",
    "        return self.train_data, self.val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded chars in train: [49 51 41 14  3 25 64 51  3 51  5 57 53 37 44  4 57 66 29 41 57 25 33 57\n",
      " 25  6 41 29 21 57 57 42 25 36 53 31 25 66 11 41  3  7 57 41 20 25  7 57\n",
      " 36 41 25 13 57 25 14  6 57 36 10 38 44 44 22 23 23 37 44 30  6 57 36 10\n",
      " 20 25 14  6 57 36 10 38 44 44 49 51 41 14  3 25 64 51  3 51  5 57 53 37\n",
      " 44 28 29 11]\n",
      "Number of chars in vocab:  67\n",
      "Train text:  First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "data = Dataset()\n",
    "train_data, val_data = data.get_data()\n",
    "print(\"Encoded chars in train:\", train_data[:100])\n",
    "print(\"Number of chars in vocab: \", len(data.chars))\n",
    "print(\"Train text: \", data.text_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    '''\n",
    "    Load data from dataset in batches (batches = n_seqs * n_steps)\n",
    "    '''\n",
    "    def __init__(self, train, val):\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, arr, n_seqs, n_steps):\n",
    "        '''\n",
    "        Create a generator that returns batches of size\n",
    "        n_seqs x n_steps from arr.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        arr: np.array\n",
    "            Array you want to make batches from\n",
    "        n_seqs: int\n",
    "            Batch size, the number of sequences per batch\n",
    "        n_steps: int\n",
    "            Number of sequence steps per batch\n",
    "        '''\n",
    "        batch_size = n_seqs * n_steps\n",
    "        n_batches = len(arr) // batch_size\n",
    "        arr = arr[:n_batches * batch_size]\n",
    "        arr = arr.reshape((n_seqs, -1))\n",
    "        \n",
    "        for n in range(0, arr.shape[1], n_steps):\n",
    "            x = arr[:, n: n + n_steps]\n",
    "            y = np.zeros_like(x)\n",
    "            try:\n",
    "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
    "            except IndexError:\n",
    "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[49, 51, 41, 14,  3]]), array([[51, 41, 14,  3, 25]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = DataLoader(train_data, val_data)\n",
    "next(data_loader(train_data, 1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_steps=Config.n_steps, n_hidden=Config.n_hidden, n_layers=Config.n_layers,\n",
    "                    drop_prob=Config.dropout, lr=Config.lr):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr      \n",
    "        self.lstm = nn.LSTM(vocab_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)        \n",
    "        self.dropout = nn.Dropout(drop_prob)      \n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' \n",
    "        Initialize weights for fully connected layer \n",
    "        '''\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' \n",
    "        Initializes hidden state \n",
    "        '''\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        ''' \n",
    "        Forward pass through the network. \n",
    "        These inputs are x, and the hidden/cell state `hc`. \n",
    "        '''\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = x.reshape(x.size()[0] * x.size()[1], self.n_hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data, val_data, epochs=Config.epochs, n_seqs=Config.n_seqs, \n",
    "          n_steps=Config.n_steps, lr=Config.lr, clip=Config.clip, cuda=Config.cuda):\n",
    "    ''' \n",
    "        Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ----------------\n",
    "        net: RNN network\n",
    "        train_data: text data to train the network\n",
    "        val_data: text data to validate the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        cuda: Train with CUDA on a GPU\n",
    "    '''\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Early stopping\n",
    "    the_last_loss = 100\n",
    "    patience = 10\n",
    "    trigger_times = 0\n",
    "    isStopped = False\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        if isStopped:\n",
    "            break\n",
    "        for x, y in data_loader(train_data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = data.one_hot_encode(x, net.vocab_size)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % 10 == 0:\n",
    "                \n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in data_loader(val_data, n_seqs, n_steps):\n",
    "                    x = data.one_hot_encode(x, net.vocab_size)\n",
    "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "\n",
    "                the_current_loss = np.mean(val_losses)\n",
    "                if the_current_loss > the_last_loss:\n",
    "                    trigger_times += 1\n",
    "                    print('trigger times: ', trigger_times)\n",
    "                    if trigger_times >= patience:\n",
    "                        print('Early stopping! at epoch {0}'.format(e))\n",
    "                        isStopped = True\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    print('trigger times: 0')\n",
    "                    trigger_times = 0\n",
    "                    the_last_loss = the_current_loss\n",
    "                    if not isStopped:\n",
    "                        with open('models/rnn.net', 'wb') as f:\n",
    "                            torch.save(net.state_dict(), f)\n",
    "                        print('Validation loss {:.6f}.  Saving model ...'.format(the_current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(67, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=67, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "net = RNN(vocab_size=len(data.chars))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.4359... Val Loss: 3.4042\n",
      "trigger times: 0\n",
      "Validation loss 3.404189.  Saving model ...\n",
      "Epoch: 1/1... Step: 20... Loss: 3.3170... Val Loss: 3.2866\n",
      "trigger times: 0\n",
      "Validation loss 3.286617.  Saving model ...\n",
      "Epoch: 1/1... Step: 30... Loss: 3.1480... Val Loss: 3.1415\n",
      "trigger times: 0\n",
      "Validation loss 3.141513.  Saving model ...\n",
      "Epoch: 1/1... Step: 40... Loss: 2.9886... Val Loss: 2.9680\n",
      "trigger times: 0\n",
      "Validation loss 2.967990.  Saving model ...\n",
      "Epoch: 1/1... Step: 50... Loss: 2.7885... Val Loss: 2.7829\n",
      "trigger times: 0\n",
      "Validation loss 2.782930.  Saving model ...\n",
      "Epoch: 1/1... Step: 60... Loss: 2.6836... Val Loss: 2.6688\n",
      "trigger times: 0\n",
      "Validation loss 2.668850.  Saving model ...\n",
      "Epoch: 1/1... Step: 70... Loss: 2.5884... Val Loss: 2.5943\n",
      "trigger times: 0\n",
      "Validation loss 2.594312.  Saving model ...\n",
      "Epoch: 1/1... Step: 80... Loss: 2.5724... Val Loss: 2.5370\n",
      "trigger times: 0\n",
      "Validation loss 2.536999.  Saving model ...\n",
      "Epoch: 1/1... Step: 90... Loss: 2.4969... Val Loss: 2.4868\n",
      "trigger times: 0\n",
      "Validation loss 2.486801.  Saving model ...\n",
      "Epoch: 1/1... Step: 100... Loss: 2.4635... Val Loss: 2.4518\n",
      "trigger times: 0\n",
      "Validation loss 2.451842.  Saving model ...\n",
      "Epoch: 1/1... Step: 110... Loss: 2.4066... Val Loss: 2.4225\n",
      "trigger times: 0\n",
      "Validation loss 2.422486.  Saving model ...\n",
      "Epoch: 1/1... Step: 120... Loss: 2.3713... Val Loss: 2.3900\n",
      "trigger times: 0\n",
      "Validation loss 2.389979.  Saving model ...\n",
      "Epoch: 1/1... Step: 130... Loss: 2.3466... Val Loss: 2.3693\n",
      "trigger times: 0\n",
      "Validation loss 2.369264.  Saving model ...\n",
      "Epoch: 1/1... Step: 140... Loss: 2.3242... Val Loss: 2.3490\n",
      "trigger times: 0\n",
      "Validation loss 2.349006.  Saving model ...\n",
      "Epoch: 1/1... Step: 150... Loss: 2.3088... Val Loss: 2.3317\n",
      "trigger times: 0\n",
      "Validation loss 2.331746.  Saving model ...\n",
      "Epoch: 1/1... Step: 160... Loss: 2.3021... Val Loss: 2.3090\n",
      "trigger times: 0\n",
      "Validation loss 2.308974.  Saving model ...\n",
      "Epoch: 1/1... Step: 170... Loss: 2.2984... Val Loss: 2.2863\n",
      "trigger times: 0\n",
      "Validation loss 2.286322.  Saving model ...\n",
      "Epoch: 1/1... Step: 180... Loss: 2.2340... Val Loss: 2.2734\n",
      "trigger times: 0\n",
      "Validation loss 2.273430.  Saving model ...\n",
      "Epoch: 1/1... Step: 190... Loss: 2.2368... Val Loss: 2.2569\n",
      "trigger times: 0\n",
      "Validation loss 2.256933.  Saving model ...\n",
      "Epoch: 1/1... Step: 200... Loss: 2.2041... Val Loss: 2.2340\n",
      "trigger times: 0\n",
      "Validation loss 2.234005.  Saving model ...\n",
      "Epoch: 1/1... Step: 210... Loss: 2.1958... Val Loss: 2.2259\n",
      "trigger times: 0\n",
      "Validation loss 2.225878.  Saving model ...\n",
      "Epoch: 1/1... Step: 220... Loss: 2.2370... Val Loss: 2.2179\n",
      "trigger times: 0\n",
      "Validation loss 2.217945.  Saving model ...\n",
      "Epoch: 1/1... Step: 230... Loss: 2.1695... Val Loss: 2.2006\n",
      "trigger times: 0\n",
      "Validation loss 2.200558.  Saving model ...\n",
      "Epoch: 1/1... Step: 240... Loss: 2.1675... Val Loss: 2.1886\n",
      "trigger times: 0\n",
      "Validation loss 2.188553.  Saving model ...\n",
      "Epoch: 1/1... Step: 250... Loss: 2.1199... Val Loss: 2.1792\n",
      "trigger times: 0\n",
      "Validation loss 2.179177.  Saving model ...\n",
      "Epoch: 1/1... Step: 260... Loss: 2.1247... Val Loss: 2.1673\n",
      "trigger times: 0\n",
      "Validation loss 2.167323.  Saving model ...\n",
      "Epoch: 1/1... Step: 270... Loss: 2.0858... Val Loss: 2.1559\n",
      "trigger times: 0\n",
      "Validation loss 2.155928.  Saving model ...\n",
      "Epoch: 1/1... Step: 280... Loss: 2.1101... Val Loss: 2.1416\n",
      "trigger times: 0\n",
      "Validation loss 2.141614.  Saving model ...\n",
      "Epoch: 1/1... Step: 290... Loss: 2.0696... Val Loss: 2.1238\n",
      "trigger times: 0\n",
      "Validation loss 2.123793.  Saving model ...\n",
      "Epoch: 1/1... Step: 300... Loss: 2.1354... Val Loss: 2.1417\n",
      "trigger times:  1\n",
      "Epoch: 1/1... Step: 310... Loss: 2.0793... Val Loss: 2.1151\n",
      "trigger times: 0\n",
      "Validation loss 2.115066.  Saving model ...\n",
      "Epoch: 1/1... Step: 320... Loss: 2.0858... Val Loss: 2.1037\n",
      "trigger times: 0\n",
      "Validation loss 2.103727.  Saving model ...\n",
      "Epoch: 1/1... Step: 330... Loss: 2.0513... Val Loss: 2.0981\n",
      "trigger times: 0\n",
      "Validation loss 2.098108.  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "train(net=net, train_data=train_data, val_data=val_data, epochs=1, n_seqs=128, n_steps=100, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            net.cuda()\n",
    "        else:\n",
    "            net.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = net.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[data.char2int[char]]])\n",
    "        x = data.one_hot_encode(x, len(data.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = net.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(data.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return data.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "    '''\n",
    "    Generate the next `size` characters from given `prime`\n",
    "    '''\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # Run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juliet as the sand, I hive bear that the sto dow me her her hound a that's a deat thou stor more my thourer a mayedst, to he will shall beand shild him be manter as is thy leed\n",
      "I he sall to be math to the manter the conteres.\n",
      "\n",
      "BOTTRES:\n",
      "And tay to here.\n",
      "\n",
      "LAOD IUS ERANA:\n",
      "That she live sear thou will the tond somer ant may him then that have hung to mont thou hather'd, and thou to bettred.'\n",
      "Thould the make have to be ang of thee her asteres mint,\n",
      "And thy wall heal the mand that the thang as the that\n",
      "Aspils, I as it took to he mist to his some sairs me that sore and and a mady fill and so me thou his beed, and stay, I'll say and a the cone of hear shere her, well thou had so ard here then all ast and the camser with my soon a may thou wollds the hiss as trat with shat an my to dast he sound and the send the mise of the foor he wand whome so has shill, sere a tour ore tressings, tore all ance stind and thy ford and where hast as ance at the wing in and the case of his hear and me hear that shill h\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Juliet', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6996669039b24f13aef7be20d9606477de00a70217801a8b6318938f205f3171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
