{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "58IKHAnb1t3d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TFDddbxPOqm_"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    '''\n",
        "    Config class defines dataset path and hyperparameters.\n",
        "    '''\n",
        "    data_train_url = 'data/shakespeare_train.txt'\n",
        "    data_val_url = 'data/shakespeare_valid.txt'\n",
        "    n_hidden = 512\n",
        "    n_layers = 2\n",
        "    epochs = 25 \n",
        "    n_seqs = 128\n",
        "    n_steps = 100\n",
        "    lr = 0.001\n",
        "    clip = 5\n",
        "    cuda = False\n",
        "    dropout = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n0zvIA_wzqV9"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    '''\n",
        "    Load data from data path, preprocess (tokenize & one-hot encode) and get data in array type.\n",
        "    '''\n",
        "    def __init__(self, data_train_url = Config.data_train_url, data_val_url = Config.data_val_url):\n",
        "        with io.open (data_train_url, 'r') as f:\n",
        "            self.text_train = f.read()\n",
        "        with io.open (data_val_url, 'r') as f:\n",
        "            self.text_val = f.read()\n",
        "\n",
        "    def char_tokenize(self):\n",
        "        self.chars = tuple(set(self.text_train))\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        self.train_data = np.array([self.char2int[ch] for ch in self.text_train])\n",
        "        self.val_data = np.array([self.char2int[ch] for ch in self.text_val])\n",
        "\n",
        "    def one_hot_encode(self, arr, n_labels):\n",
        "        one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "        one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "        one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "        return one_hot\n",
        "\n",
        "    def get_data(self):\n",
        "        self.char_tokenize()\n",
        "        return self.train_data, self.val_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "dPcc9D0-l_5H",
        "outputId": "6641aaeb-c207-4893-9fc7-dd29d48d4262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[52 35 64  7 58 24 11 35 58 35 60 29 36 22 20  1 29 65 28 64 29 24 39 29\n",
            " 24 21 64 28 41 29 29 59 24 51 36 38 24 65 54 64 58  4 29 64 55 24  4 29\n",
            " 51 64 24 31 29 24  7 21 29 51 42 30 20 20 32 23 23 22 20 40 21 29 51 42\n",
            " 55 24  7 21 29 51 42 30 20 20 52 35 64  7 58 24 11 35 58 35 60 29 36 22\n",
            " 20 13 28 54]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = Dataset()\n",
        "train_data, val_data = data.get_data()\n",
        "print(\"Encoded chars in train:\", train_data[:100])\n",
        "print(\"Number of chars in vocab: \", len(data.chars))\n",
        "print(\"Train text: \", data.text_train[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mLo_OyF1zeJA"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    '''\n",
        "    Load data from dataset in batches (batches = n_seqs * n_steps)\n",
        "    '''\n",
        "    def __init__(self, train, val):\n",
        "        self.train = train\n",
        "        self.val = val\n",
        "\n",
        "    def __call__(self, arr, n_seqs, n_steps):\n",
        "        '''\n",
        "        Create a generator that returns batches of size\n",
        "        n_seqs x n_steps from arr.\n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        arr: np.array\n",
        "            Array you want to make batches from\n",
        "        n_seqs: int\n",
        "            Batch size, the number of sequences per batch\n",
        "        n_steps: int\n",
        "            Number of sequence steps per batch\n",
        "        '''\n",
        "        batch_size = n_seqs * n_steps\n",
        "        n_batches = len(arr) // batch_size\n",
        "        arr = arr[:n_batches * batch_size]\n",
        "        arr = arr.reshape((n_seqs, -1))\n",
        "        \n",
        "        for n in range(0, arr.shape[1], n_steps):\n",
        "            x = arr[:, n: n + n_steps]\n",
        "            y = np.zeros_like(x)\n",
        "            try:\n",
        "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
        "            except IndexError:\n",
        "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "            yield x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vyGsVqPKl4rz"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(train_data, val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3YTLgNuLLPZQ"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, n_steps=Config.n_steps, n_hidden=Config.n_hidden, n_layers=Config.n_layers,\n",
        "                    drop_prob=Config.dropout, lr=Config.lr):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr      \n",
        "        self.lstm = nn.LSTM(vocab_size, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)        \n",
        "        self.dropout = nn.Dropout(drop_prob)      \n",
        "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        ''' \n",
        "        Initialize weights for fully connected layer \n",
        "        '''\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-1, 1)\n",
        "        \n",
        "    def init_hidden(self, n_seqs):\n",
        "        ''' \n",
        "        Initializes hidden state \n",
        "        '''\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
        "\n",
        "    def forward(self, x, hc):\n",
        "        ''' \n",
        "        Forward pass through the network. \n",
        "        These inputs are x, and the hidden/cell state `hc`. \n",
        "        '''\n",
        "        x, (h, c) = self.lstm(x, hc)\n",
        "        x = self.dropout(x)\n",
        "        x = x.reshape(x.size()[0] * x.size()[1], self.n_hidden)\n",
        "        x = self.fc(x)\n",
        "        return x, (h, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OhBmZlo9jhDC"
      },
      "outputs": [],
      "source": [
        "def train(net, train_data, val_data, epochs=Config.epochs, n_seqs=Config.n_seqs, \n",
        "          n_steps=Config.n_steps, lr=Config.lr, clip=Config.clip, cuda=Config.cuda):\n",
        "    ''' \n",
        "        Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ----------------\n",
        "        net: RNN network\n",
        "        train_data: text data to train the network\n",
        "        val_data: text data to validate the network\n",
        "        epochs: Number of epochs to train\n",
        "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
        "        n_steps: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        cuda: Train with CUDA on a GPU\n",
        "    '''\n",
        "    net.train()\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping\n",
        "    the_last_loss = 100\n",
        "    patience = 10\n",
        "    trigger_times = 0\n",
        "    isStopped = False\n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    for e in range(epochs):\n",
        "        h = net.init_hidden(n_seqs)\n",
        "        if isStopped:\n",
        "            break\n",
        "        for x, y in data_loader(train_data, n_seqs, n_steps):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = data.one_hot_encode(x, net.vocab_size)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, h = net.forward(inputs, h)\n",
        "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "\n",
        "            opt.step()\n",
        "            \n",
        "            if counter % 10 == 0:\n",
        "                \n",
        "                val_h = net.init_hidden(n_seqs)\n",
        "                val_losses = []\n",
        "                for x, y in data_loader(val_data, n_seqs, n_steps):\n",
        "                    x = data.one_hot_encode(x, net.vocab_size)\n",
        "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    if cuda:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    output, val_h = net.forward(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "\n",
        "                the_current_loss = np.mean(val_losses)\n",
        "                if the_current_loss > the_last_loss:\n",
        "                    trigger_times += 1\n",
        "                    print('trigger times: ', trigger_times)\n",
        "                    if trigger_times >= patience:\n",
        "                        print('Early stopping! at epoch {0}'.format(e))\n",
        "                        isStopped = True\n",
        "                        break\n",
        "\n",
        "                else:\n",
        "                    print('trigger times: 0')\n",
        "                    trigger_times = 0\n",
        "                    the_last_loss = the_current_loss\n",
        "                    if not isStopped:\n",
        "                        with open('models/rnn.net', 'wb') as f:\n",
        "                            torch.save(net.state_dict(), f)\n",
        "                        print('Validation loss {:.6f}.  Saving model ...'.format(the_current_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jfuaaop3pDQB"
      },
      "outputs": [],
      "source": [
        "if 'net' in locals():\n",
        "    del net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19u0QCBlpIIC",
        "outputId": "fc0652d7-95cf-4713-890f-41a678c5a5ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data.chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPYD8ES9pPgg",
        "outputId": "92f78282-9bc9-49b4-bcde-0fc02dd7ed0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (lstm): LSTM(67, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=67, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# define and print the net\n",
        "net = RNN(input_size=len(data.chars))\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcMzEo87p4Fz",
        "outputId": "fa08c053-a442-4905-bbc8-6cd3dc4e8c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/25... Step: 10... Loss: 3.4277... Val Loss: 3.3981\n",
            "trigger times: 0\n",
            "Validation loss 3.398069.  Saving model ...\n",
            "Epoch: 1/25... Step: 20... Loss: 3.2862... Val Loss: 3.2549\n",
            "trigger times: 0\n",
            "Validation loss 3.254935.  Saving model ...\n",
            "Epoch: 1/25... Step: 30... Loss: 3.1153... Val Loss: 3.0989\n",
            "trigger times: 0\n",
            "Validation loss 3.098858.  Saving model ...\n",
            "Epoch: 1/25... Step: 40... Loss: 2.9333... Val Loss: 2.9153\n",
            "trigger times: 0\n",
            "Validation loss 2.915315.  Saving model ...\n",
            "Epoch: 1/25... Step: 50... Loss: 2.7370... Val Loss: 2.7431\n",
            "trigger times: 0\n",
            "Validation loss 2.743079.  Saving model ...\n",
            "Epoch: 1/25... Step: 60... Loss: 2.6336... Val Loss: 2.6223\n",
            "trigger times: 0\n",
            "Validation loss 2.622278.  Saving model ...\n",
            "Epoch: 1/25... Step: 70... Loss: 2.5378... Val Loss: 2.5502\n",
            "trigger times: 0\n",
            "Validation loss 2.550151.  Saving model ...\n",
            "Epoch: 1/25... Step: 80... Loss: 2.5215... Val Loss: 2.4971\n",
            "trigger times: 0\n",
            "Validation loss 2.497054.  Saving model ...\n",
            "Epoch: 1/25... Step: 90... Loss: 2.4737... Val Loss: 2.4607\n",
            "trigger times: 0\n",
            "Validation loss 2.460678.  Saving model ...\n",
            "Epoch: 1/25... Step: 100... Loss: 2.4312... Val Loss: 2.4262\n",
            "trigger times: 0\n",
            "Validation loss 2.426190.  Saving model ...\n",
            "Epoch: 1/25... Step: 110... Loss: 2.3829... Val Loss: 2.3996\n",
            "trigger times: 0\n",
            "Validation loss 2.399576.  Saving model ...\n",
            "Epoch: 1/25... Step: 120... Loss: 2.3455... Val Loss: 2.3679\n",
            "trigger times: 0\n",
            "Validation loss 2.367919.  Saving model ...\n",
            "Epoch: 1/25... Step: 130... Loss: 2.3278... Val Loss: 2.3416\n",
            "trigger times: 0\n",
            "Validation loss 2.341562.  Saving model ...\n",
            "Epoch: 1/25... Step: 140... Loss: 2.2973... Val Loss: 2.3262\n",
            "trigger times: 0\n",
            "Validation loss 2.326247.  Saving model ...\n",
            "Epoch: 1/25... Step: 150... Loss: 2.2899... Val Loss: 2.3111\n",
            "trigger times: 0\n",
            "Validation loss 2.311133.  Saving model ...\n",
            "Epoch: 1/25... Step: 160... Loss: 2.2841... Val Loss: 2.2894\n",
            "trigger times: 0\n",
            "Validation loss 2.289367.  Saving model ...\n",
            "Epoch: 1/25... Step: 170... Loss: 2.2811... Val Loss: 2.2731\n",
            "trigger times: 0\n",
            "Validation loss 2.273139.  Saving model ...\n",
            "Epoch: 1/25... Step: 180... Loss: 2.2231... Val Loss: 2.2573\n",
            "trigger times: 0\n",
            "Validation loss 2.257258.  Saving model ...\n",
            "Epoch: 1/25... Step: 190... Loss: 2.2135... Val Loss: 2.2425\n",
            "trigger times: 0\n",
            "Validation loss 2.242516.  Saving model ...\n",
            "Epoch: 1/25... Step: 200... Loss: 2.1999... Val Loss: 2.2223\n",
            "trigger times: 0\n",
            "Validation loss 2.222292.  Saving model ...\n",
            "Epoch: 1/25... Step: 210... Loss: 2.1817... Val Loss: 2.2094\n",
            "trigger times: 0\n",
            "Validation loss 2.209441.  Saving model ...\n",
            "Epoch: 1/25... Step: 220... Loss: 2.2145... Val Loss: 2.2009\n",
            "trigger times: 0\n",
            "Validation loss 2.200888.  Saving model ...\n",
            "Epoch: 1/25... Step: 230... Loss: 2.1589... Val Loss: 2.1829\n",
            "trigger times: 0\n",
            "Validation loss 2.182854.  Saving model ...\n",
            "Epoch: 1/25... Step: 240... Loss: 2.1507... Val Loss: 2.1716\n",
            "trigger times: 0\n",
            "Validation loss 2.171554.  Saving model ...\n",
            "Epoch: 1/25... Step: 250... Loss: 2.1088... Val Loss: 2.1609\n",
            "trigger times: 0\n",
            "Validation loss 2.160860.  Saving model ...\n",
            "Epoch: 1/25... Step: 260... Loss: 2.1183... Val Loss: 2.1584\n",
            "trigger times: 0\n",
            "Validation loss 2.158362.  Saving model ...\n",
            "Epoch: 1/25... Step: 270... Loss: 2.0788... Val Loss: 2.1422\n",
            "trigger times: 0\n",
            "Validation loss 2.142192.  Saving model ...\n",
            "Epoch: 1/25... Step: 280... Loss: 2.1052... Val Loss: 2.1289\n",
            "trigger times: 0\n",
            "Validation loss 2.128858.  Saving model ...\n",
            "Epoch: 1/25... Step: 290... Loss: 2.0590... Val Loss: 2.1141\n",
            "trigger times: 0\n",
            "Validation loss 2.114131.  Saving model ...\n",
            "Epoch: 1/25... Step: 300... Loss: 2.0947... Val Loss: 2.1055\n",
            "trigger times: 0\n",
            "Validation loss 2.105487.  Saving model ...\n",
            "Epoch: 1/25... Step: 310... Loss: 2.0503... Val Loss: 2.0919\n",
            "trigger times: 0\n",
            "Validation loss 2.091866.  Saving model ...\n",
            "Epoch: 1/25... Step: 320... Loss: 2.0545... Val Loss: 2.0884\n",
            "trigger times: 0\n",
            "Validation loss 2.088441.  Saving model ...\n",
            "Epoch: 1/25... Step: 330... Loss: 2.0237... Val Loss: 2.0791\n",
            "trigger times: 0\n",
            "Validation loss 2.079136.  Saving model ...\n",
            "Epoch: 2/25... Step: 340... Loss: 2.0705... Val Loss: 2.0673\n",
            "trigger times: 0\n",
            "Validation loss 2.067298.  Saving model ...\n",
            "Epoch: 2/25... Step: 350... Loss: 2.0337... Val Loss: 2.0607\n",
            "trigger times: 0\n",
            "Validation loss 2.060715.  Saving model ...\n",
            "Epoch: 2/25... Step: 360... Loss: 2.0144... Val Loss: 2.0456\n",
            "trigger times: 0\n",
            "Validation loss 2.045565.  Saving model ...\n",
            "Epoch: 2/25... Step: 370... Loss: 1.9835... Val Loss: 2.0380\n",
            "trigger times: 0\n",
            "Validation loss 2.037968.  Saving model ...\n",
            "Epoch: 2/25... Step: 380... Loss: 2.0061... Val Loss: 2.0438\n",
            "trigger times:  1\n",
            "Epoch: 2/25... Step: 390... Loss: 1.9768... Val Loss: 2.0217\n",
            "trigger times: 0\n",
            "Validation loss 2.021656.  Saving model ...\n",
            "Epoch: 2/25... Step: 400... Loss: 1.9913... Val Loss: 2.0148\n",
            "trigger times: 0\n",
            "Validation loss 2.014769.  Saving model ...\n",
            "Epoch: 2/25... Step: 410... Loss: 1.9754... Val Loss: 2.0035\n",
            "trigger times: 0\n",
            "Validation loss 2.003522.  Saving model ...\n",
            "Epoch: 2/25... Step: 420... Loss: 1.9446... Val Loss: 2.0187\n",
            "trigger times:  1\n",
            "Epoch: 2/25... Step: 430... Loss: 1.9660... Val Loss: 1.9992\n",
            "trigger times: 0\n",
            "Validation loss 1.999171.  Saving model ...\n",
            "Epoch: 2/25... Step: 440... Loss: 1.9075... Val Loss: 1.9901\n",
            "trigger times: 0\n",
            "Validation loss 1.990058.  Saving model ...\n",
            "Epoch: 2/25... Step: 450... Loss: 1.9384... Val Loss: 1.9813\n",
            "trigger times: 0\n",
            "Validation loss 1.981282.  Saving model ...\n",
            "Epoch: 2/25... Step: 460... Loss: 1.8971... Val Loss: 1.9736\n",
            "trigger times: 0\n",
            "Validation loss 1.973597.  Saving model ...\n",
            "Epoch: 2/25... Step: 470... Loss: 1.9314... Val Loss: 1.9725\n",
            "trigger times: 0\n",
            "Validation loss 1.972474.  Saving model ...\n",
            "Epoch: 2/25... Step: 480... Loss: 1.9182... Val Loss: 1.9635\n",
            "trigger times: 0\n",
            "Validation loss 1.963466.  Saving model ...\n",
            "Epoch: 2/25... Step: 490... Loss: 1.8913... Val Loss: 1.9569\n",
            "trigger times: 0\n",
            "Validation loss 1.956905.  Saving model ...\n",
            "Epoch: 2/25... Step: 500... Loss: 1.8952... Val Loss: 1.9597\n",
            "trigger times:  1\n",
            "Epoch: 2/25... Step: 510... Loss: 1.9260... Val Loss: 1.9468\n",
            "trigger times: 0\n",
            "Validation loss 1.946781.  Saving model ...\n",
            "Epoch: 2/25... Step: 520... Loss: 1.8956... Val Loss: 1.9335\n",
            "trigger times: 0\n",
            "Validation loss 1.933452.  Saving model ...\n",
            "Epoch: 2/25... Step: 530... Loss: 1.8768... Val Loss: 1.9365\n",
            "trigger times:  1\n",
            "Epoch: 2/25... Step: 540... Loss: 1.8599... Val Loss: 1.9201\n",
            "trigger times: 0\n",
            "Validation loss 1.920102.  Saving model ...\n",
            "Epoch: 2/25... Step: 550... Loss: 1.8695... Val Loss: 1.9181\n",
            "trigger times: 0\n",
            "Validation loss 1.918052.  Saving model ...\n",
            "Epoch: 2/25... Step: 560... Loss: 1.8570... Val Loss: 1.9189\n",
            "trigger times:  1\n",
            "Epoch: 2/25... Step: 570... Loss: 1.8049... Val Loss: 1.9147\n",
            "trigger times: 0\n",
            "Validation loss 1.914725.  Saving model ...\n",
            "Epoch: 2/25... Step: 580... Loss: 1.8188... Val Loss: 1.9068\n",
            "trigger times: 0\n",
            "Validation loss 1.906825.  Saving model ...\n",
            "Epoch: 2/25... Step: 590... Loss: 1.8321... Val Loss: 1.9102\n",
            "trigger times:  1\n",
            "Epoch: 2/25... Step: 600... Loss: 1.8196... Val Loss: 1.9005\n",
            "trigger times: 0\n",
            "Validation loss 1.900470.  Saving model ...\n",
            "Epoch: 2/25... Step: 610... Loss: 1.8175... Val Loss: 1.8926\n",
            "trigger times: 0\n",
            "Validation loss 1.892639.  Saving model ...\n",
            "Epoch: 2/25... Step: 620... Loss: 1.8119... Val Loss: 1.8903\n",
            "trigger times: 0\n",
            "Validation loss 1.890291.  Saving model ...\n",
            "Epoch: 2/25... Step: 630... Loss: 1.8130... Val Loss: 1.8877\n",
            "trigger times: 0\n",
            "Validation loss 1.887691.  Saving model ...\n",
            "Epoch: 2/25... Step: 640... Loss: 1.8163... Val Loss: 1.8775\n",
            "trigger times: 0\n",
            "Validation loss 1.877538.  Saving model ...\n",
            "Epoch: 2/25... Step: 650... Loss: 1.7972... Val Loss: 1.8779\n",
            "trigger times:  1\n",
            "Epoch: 2/25... Step: 660... Loss: 1.7963... Val Loss: 1.8698\n",
            "trigger times: 0\n",
            "Validation loss 1.869840.  Saving model ...\n",
            "Epoch: 2/25... Step: 670... Loss: 1.7786... Val Loss: 1.8646\n",
            "trigger times: 0\n",
            "Validation loss 1.864621.  Saving model ...\n",
            "Epoch: 3/25... Step: 680... Loss: 1.7789... Val Loss: 1.8657\n",
            "trigger times:  1\n",
            "Epoch: 3/25... Step: 690... Loss: 1.7762... Val Loss: 1.8557\n",
            "trigger times: 0\n",
            "Validation loss 1.855748.  Saving model ...\n",
            "Epoch: 3/25... Step: 700... Loss: 1.7659... Val Loss: 1.8618\n",
            "trigger times:  1\n",
            "Epoch: 3/25... Step: 710... Loss: 1.7870... Val Loss: 1.8484\n",
            "trigger times: 0\n",
            "Validation loss 1.848375.  Saving model ...\n",
            "Epoch: 3/25... Step: 720... Loss: 1.7479... Val Loss: 1.8460\n",
            "trigger times: 0\n",
            "Validation loss 1.846026.  Saving model ...\n",
            "Epoch: 3/25... Step: 730... Loss: 1.7586... Val Loss: 1.8438\n",
            "trigger times: 0\n",
            "Validation loss 1.843759.  Saving model ...\n",
            "Epoch: 3/25... Step: 740... Loss: 1.7676... Val Loss: 1.8364\n",
            "trigger times: 0\n",
            "Validation loss 1.836372.  Saving model ...\n",
            "Epoch: 3/25... Step: 750... Loss: 1.7302... Val Loss: 1.8394\n",
            "trigger times:  1\n",
            "Epoch: 3/25... Step: 760... Loss: 1.7706... Val Loss: 1.8355\n",
            "trigger times: 0\n",
            "Validation loss 1.835467.  Saving model ...\n",
            "Epoch: 3/25... Step: 770... Loss: 1.7773... Val Loss: 1.8345\n",
            "trigger times: 0\n",
            "Validation loss 1.834514.  Saving model ...\n",
            "Epoch: 3/25... Step: 780... Loss: 1.7480... Val Loss: 1.8265\n",
            "trigger times: 0\n",
            "Validation loss 1.826535.  Saving model ...\n",
            "Epoch: 3/25... Step: 790... Loss: 1.7091... Val Loss: 1.8267\n",
            "trigger times:  1\n",
            "Epoch: 3/25... Step: 800... Loss: 1.7181... Val Loss: 1.8215\n",
            "trigger times: 0\n",
            "Validation loss 1.821540.  Saving model ...\n",
            "Epoch: 3/25... Step: 810... Loss: 1.7181... Val Loss: 1.8324\n",
            "trigger times:  1\n",
            "Epoch: 3/25... Step: 820... Loss: 1.7252... Val Loss: 1.8173\n",
            "trigger times: 0\n",
            "Validation loss 1.817298.  Saving model ...\n",
            "Epoch: 3/25... Step: 830... Loss: 1.7259... Val Loss: 1.8294\n",
            "trigger times:  1\n",
            "Epoch: 3/25... Step: 840... Loss: 1.7415... Val Loss: 1.8097\n",
            "trigger times: 0\n",
            "Validation loss 1.809658.  Saving model ...\n",
            "Epoch: 3/25... Step: 850... Loss: 1.7150... Val Loss: 1.8089\n",
            "trigger times: 0\n",
            "Validation loss 1.808926.  Saving model ...\n",
            "Epoch: 3/25... Step: 860... Loss: 1.7237... Val Loss: 1.8080\n",
            "trigger times: 0\n",
            "Validation loss 1.807972.  Saving model ...\n",
            "Epoch: 3/25... Step: 870... Loss: 1.7191... Val Loss: 1.8142\n",
            "trigger times:  1\n",
            "Epoch: 3/25... Step: 880... Loss: 1.7166... Val Loss: 1.7984\n",
            "trigger times: 0\n",
            "Validation loss 1.798437.  Saving model ...\n",
            "Epoch: 3/25... Step: 890... Loss: 1.6961... Val Loss: 1.7889\n",
            "trigger times: 0\n",
            "Validation loss 1.788914.  Saving model ...\n",
            "Epoch: 3/25... Step: 900... Loss: 1.6810... Val Loss: 1.7921\n",
            "trigger times:  1\n",
            "Epoch: 3/25... Step: 910... Loss: 1.6763... Val Loss: 1.7979\n",
            "trigger times:  2\n",
            "Epoch: 3/25... Step: 920... Loss: 1.6669... Val Loss: 1.7905\n",
            "trigger times:  3\n",
            "Epoch: 3/25... Step: 930... Loss: 1.6571... Val Loss: 1.7896\n",
            "trigger times:  4\n",
            "Epoch: 3/25... Step: 940... Loss: 1.6815... Val Loss: 1.7976\n",
            "trigger times:  5\n",
            "Epoch: 3/25... Step: 950... Loss: 1.6838... Val Loss: 1.7885\n",
            "trigger times: 0\n",
            "Validation loss 1.788460.  Saving model ...\n",
            "Epoch: 3/25... Step: 960... Loss: 1.6480... Val Loss: 1.7882\n",
            "trigger times: 0\n",
            "Validation loss 1.788160.  Saving model ...\n",
            "Epoch: 3/25... Step: 970... Loss: 1.6277... Val Loss: 1.7917\n",
            "trigger times:  1\n",
            "Epoch: 3/25... Step: 980... Loss: 1.6718... Val Loss: 1.7841\n",
            "trigger times: 0\n",
            "Validation loss 1.784084.  Saving model ...\n",
            "Epoch: 3/25... Step: 990... Loss: 1.6635... Val Loss: 1.7682\n",
            "trigger times: 0\n",
            "Validation loss 1.768158.  Saving model ...\n",
            "Epoch: 3/25... Step: 1000... Loss: 1.6507... Val Loss: 1.7675\n",
            "trigger times: 0\n",
            "Validation loss 1.767495.  Saving model ...\n",
            "Epoch: 3/25... Step: 1010... Loss: 1.6452... Val Loss: 1.7715\n",
            "trigger times:  1\n",
            "Epoch: 4/25... Step: 1020... Loss: 1.6750... Val Loss: 1.7779\n",
            "trigger times:  2\n",
            "Epoch: 4/25... Step: 1030... Loss: 1.6439... Val Loss: 1.7676\n",
            "trigger times:  3\n",
            "Epoch: 4/25... Step: 1040... Loss: 1.6604... Val Loss: 1.7580\n",
            "trigger times: 0\n",
            "Validation loss 1.757987.  Saving model ...\n",
            "Epoch: 4/25... Step: 1050... Loss: 1.6448... Val Loss: 1.7560\n",
            "trigger times: 0\n",
            "Validation loss 1.756010.  Saving model ...\n",
            "Epoch: 4/25... Step: 1060... Loss: 1.6286... Val Loss: 1.7679\n",
            "trigger times:  1\n",
            "Epoch: 4/25... Step: 1070... Loss: 1.6341... Val Loss: 1.7563\n",
            "trigger times:  2\n",
            "Epoch: 4/25... Step: 1080... Loss: 1.6329... Val Loss: 1.7555\n",
            "trigger times: 0\n",
            "Validation loss 1.755503.  Saving model ...\n",
            "Epoch: 4/25... Step: 1090... Loss: 1.6622... Val Loss: 1.7645\n",
            "trigger times:  1\n",
            "Epoch: 4/25... Step: 1100... Loss: 1.6346... Val Loss: 1.7543\n",
            "trigger times: 0\n",
            "Validation loss 1.754341.  Saving model ...\n",
            "Epoch: 4/25... Step: 1110... Loss: 1.6434... Val Loss: 1.7524\n",
            "trigger times: 0\n",
            "Validation loss 1.752384.  Saving model ...\n",
            "Epoch: 4/25... Step: 1120... Loss: 1.6156... Val Loss: 1.7475\n",
            "trigger times: 0\n",
            "Validation loss 1.747456.  Saving model ...\n",
            "Epoch: 4/25... Step: 1130... Loss: 1.5906... Val Loss: 1.7547\n",
            "trigger times:  1\n",
            "Epoch: 4/25... Step: 1140... Loss: 1.6115... Val Loss: 1.7416\n",
            "trigger times: 0\n",
            "Validation loss 1.741590.  Saving model ...\n",
            "Epoch: 4/25... Step: 1150... Loss: 1.6013... Val Loss: 1.7474\n",
            "trigger times:  1\n",
            "Epoch: 4/25... Step: 1160... Loss: 1.5832... Val Loss: 1.7438\n",
            "trigger times:  2\n",
            "Epoch: 4/25... Step: 1170... Loss: 1.6311... Val Loss: 1.7454\n",
            "trigger times:  3\n",
            "Epoch: 4/25... Step: 1180... Loss: 1.6272... Val Loss: 1.7446\n",
            "trigger times:  4\n",
            "Epoch: 4/25... Step: 1190... Loss: 1.6487... Val Loss: 1.7464\n",
            "trigger times:  5\n",
            "Epoch: 4/25... Step: 1200... Loss: 1.6133... Val Loss: 1.7338\n",
            "trigger times: 0\n",
            "Validation loss 1.733802.  Saving model ...\n",
            "Epoch: 4/25... Step: 1210... Loss: 1.5874... Val Loss: 1.7320\n",
            "trigger times: 0\n",
            "Validation loss 1.732001.  Saving model ...\n",
            "Epoch: 4/25... Step: 1220... Loss: 1.5874... Val Loss: 1.7298\n",
            "trigger times: 0\n",
            "Validation loss 1.729804.  Saving model ...\n",
            "Epoch: 4/25... Step: 1230... Loss: 1.6080... Val Loss: 1.7257\n",
            "trigger times: 0\n",
            "Validation loss 1.725736.  Saving model ...\n",
            "Epoch: 4/25... Step: 1240... Loss: 1.6125... Val Loss: 1.7235\n",
            "trigger times: 0\n",
            "Validation loss 1.723457.  Saving model ...\n",
            "Epoch: 4/25... Step: 1250... Loss: 1.5961... Val Loss: 1.7219\n",
            "trigger times: 0\n",
            "Validation loss 1.721945.  Saving model ...\n",
            "Epoch: 4/25... Step: 1260... Loss: 1.5884... Val Loss: 1.7161\n",
            "trigger times: 0\n",
            "Validation loss 1.716068.  Saving model ...\n",
            "Epoch: 4/25... Step: 1270... Loss: 1.5698... Val Loss: 1.7236\n",
            "trigger times:  1\n",
            "Epoch: 4/25... Step: 1280... Loss: 1.5851... Val Loss: 1.7301\n",
            "trigger times:  2\n",
            "Epoch: 4/25... Step: 1290... Loss: 1.5805... Val Loss: 1.7156\n",
            "trigger times: 0\n",
            "Validation loss 1.715570.  Saving model ...\n",
            "Epoch: 4/25... Step: 1300... Loss: 1.5750... Val Loss: 1.7157\n",
            "trigger times:  1\n",
            "Epoch: 4/25... Step: 1310... Loss: 1.5587... Val Loss: 1.7130\n",
            "trigger times: 0\n",
            "Validation loss 1.713000.  Saving model ...\n",
            "Epoch: 4/25... Step: 1320... Loss: 1.5825... Val Loss: 1.7184\n",
            "trigger times:  1\n",
            "Epoch: 4/25... Step: 1330... Loss: 1.5643... Val Loss: 1.7092\n",
            "trigger times: 0\n",
            "Validation loss 1.709151.  Saving model ...\n",
            "Epoch: 4/25... Step: 1340... Loss: 1.5675... Val Loss: 1.7052\n",
            "trigger times: 0\n",
            "Validation loss 1.705162.  Saving model ...\n",
            "Epoch: 4/25... Step: 1350... Loss: 1.5627... Val Loss: 1.7153\n",
            "trigger times:  1\n",
            "Epoch: 5/25... Step: 1360... Loss: 1.5691... Val Loss: 1.7115\n",
            "trigger times:  2\n",
            "Epoch: 5/25... Step: 1370... Loss: 1.5511... Val Loss: 1.6989\n",
            "trigger times: 0\n",
            "Validation loss 1.698933.  Saving model ...\n",
            "Epoch: 5/25... Step: 1380... Loss: 1.5727... Val Loss: 1.6998\n",
            "trigger times:  1\n",
            "Epoch: 5/25... Step: 1390... Loss: 1.5586... Val Loss: 1.6951\n",
            "trigger times: 0\n",
            "Validation loss 1.695091.  Saving model ...\n",
            "Epoch: 5/25... Step: 1400... Loss: 1.5536... Val Loss: 1.7083\n",
            "trigger times:  1\n",
            "Epoch: 5/25... Step: 1410... Loss: 1.5795... Val Loss: 1.6909\n",
            "trigger times: 0\n",
            "Validation loss 1.690923.  Saving model ...\n",
            "Epoch: 5/25... Step: 1420... Loss: 1.5812... Val Loss: 1.6878\n",
            "trigger times: 0\n",
            "Validation loss 1.687791.  Saving model ...\n",
            "Epoch: 5/25... Step: 1430... Loss: 1.5606... Val Loss: 1.6987\n",
            "trigger times:  1\n",
            "Epoch: 5/25... Step: 1440... Loss: 1.5506... Val Loss: 1.6995\n",
            "trigger times:  2\n",
            "Epoch: 5/25... Step: 1450... Loss: 1.5729... Val Loss: 1.6972\n",
            "trigger times:  3\n",
            "Epoch: 5/25... Step: 1460... Loss: 1.5436... Val Loss: 1.7023\n",
            "trigger times:  4\n",
            "Epoch: 5/25... Step: 1470... Loss: 1.5418... Val Loss: 1.7040\n",
            "trigger times:  5\n",
            "Epoch: 5/25... Step: 1480... Loss: 1.5594... Val Loss: 1.6907\n",
            "trigger times:  6\n",
            "Epoch: 5/25... Step: 1490... Loss: 1.5733... Val Loss: 1.6991\n",
            "trigger times:  7\n",
            "Epoch: 5/25... Step: 1500... Loss: 1.5469... Val Loss: 1.6865\n",
            "trigger times: 0\n",
            "Validation loss 1.686479.  Saving model ...\n",
            "Epoch: 5/25... Step: 1510... Loss: 1.5474... Val Loss: 1.6839\n",
            "trigger times: 0\n",
            "Validation loss 1.683948.  Saving model ...\n",
            "Epoch: 5/25... Step: 1520... Loss: 1.5665... Val Loss: 1.6915\n",
            "trigger times:  1\n",
            "Epoch: 5/25... Step: 1530... Loss: 1.5602... Val Loss: 1.6878\n",
            "trigger times:  2\n",
            "Epoch: 5/25... Step: 1540... Loss: 1.5364... Val Loss: 1.6788\n",
            "trigger times: 0\n",
            "Validation loss 1.678806.  Saving model ...\n",
            "Epoch: 5/25... Step: 1550... Loss: 1.5293... Val Loss: 1.6841\n",
            "trigger times:  1\n",
            "Epoch: 5/25... Step: 1560... Loss: 1.5914... Val Loss: 1.6840\n",
            "trigger times:  2\n",
            "Epoch: 5/25... Step: 1570... Loss: 1.5272... Val Loss: 1.6872\n",
            "trigger times:  3\n",
            "Epoch: 5/25... Step: 1580... Loss: 1.5240... Val Loss: 1.6813\n",
            "trigger times:  4\n",
            "Epoch: 5/25... Step: 1590... Loss: 1.5088... Val Loss: 1.6813\n",
            "trigger times:  5\n",
            "Epoch: 5/25... Step: 1600... Loss: 1.5108... Val Loss: 1.6823\n",
            "trigger times:  6\n",
            "Epoch: 5/25... Step: 1610... Loss: 1.5243... Val Loss: 1.6813\n",
            "trigger times:  7\n",
            "Epoch: 5/25... Step: 1620... Loss: 1.5480... Val Loss: 1.6756\n",
            "trigger times: 0\n",
            "Validation loss 1.675633.  Saving model ...\n",
            "Epoch: 5/25... Step: 1630... Loss: 1.5409... Val Loss: 1.6686\n",
            "trigger times: 0\n",
            "Validation loss 1.668619.  Saving model ...\n",
            "Epoch: 5/25... Step: 1640... Loss: 1.5107... Val Loss: 1.6784\n",
            "trigger times:  1\n",
            "Epoch: 5/25... Step: 1650... Loss: 1.5177... Val Loss: 1.6748\n",
            "trigger times:  2\n",
            "Epoch: 5/25... Step: 1660... Loss: 1.5073... Val Loss: 1.6756\n",
            "trigger times:  3\n",
            "Epoch: 5/25... Step: 1670... Loss: 1.5126... Val Loss: 1.6698\n",
            "trigger times:  4\n",
            "Epoch: 5/25... Step: 1680... Loss: 1.4989... Val Loss: 1.6564\n",
            "trigger times: 0\n",
            "Validation loss 1.656386.  Saving model ...\n",
            "Epoch: 5/25... Step: 1690... Loss: 1.4866... Val Loss: 1.6678\n",
            "trigger times:  1\n",
            "Epoch: 6/25... Step: 1700... Loss: 1.5246... Val Loss: 1.6744\n",
            "trigger times:  2\n",
            "Epoch: 6/25... Step: 1710... Loss: 1.5022... Val Loss: 1.6666\n",
            "trigger times:  3\n",
            "Epoch: 6/25... Step: 1720... Loss: 1.5013... Val Loss: 1.6756\n",
            "trigger times:  4\n",
            "Epoch: 6/25... Step: 1730... Loss: 1.5199... Val Loss: 1.6610\n",
            "trigger times:  5\n",
            "Epoch: 6/25... Step: 1740... Loss: 1.5021... Val Loss: 1.6719\n",
            "trigger times:  6\n",
            "Epoch: 6/25... Step: 1750... Loss: 1.5178... Val Loss: 1.6586\n",
            "trigger times:  7\n",
            "Epoch: 6/25... Step: 1760... Loss: 1.5109... Val Loss: 1.6540\n",
            "trigger times: 0\n",
            "Validation loss 1.654020.  Saving model ...\n",
            "Epoch: 6/25... Step: 1770... Loss: 1.5289... Val Loss: 1.6616\n",
            "trigger times:  1\n",
            "Epoch: 6/25... Step: 1780... Loss: 1.5082... Val Loss: 1.6571\n",
            "trigger times:  2\n",
            "Epoch: 6/25... Step: 1790... Loss: 1.5220... Val Loss: 1.6580\n",
            "trigger times:  3\n",
            "Epoch: 6/25... Step: 1800... Loss: 1.4941... Val Loss: 1.6603\n",
            "trigger times:  4\n",
            "Epoch: 6/25... Step: 1810... Loss: 1.4971... Val Loss: 1.6561\n",
            "trigger times:  5\n",
            "Epoch: 6/25... Step: 1820... Loss: 1.4962... Val Loss: 1.6556\n",
            "trigger times:  6\n",
            "Epoch: 6/25... Step: 1830... Loss: 1.5256... Val Loss: 1.6553\n",
            "trigger times:  7\n",
            "Epoch: 6/25... Step: 1840... Loss: 1.4881... Val Loss: 1.6560\n",
            "trigger times:  8\n",
            "Epoch: 6/25... Step: 1850... Loss: 1.4763... Val Loss: 1.6541\n",
            "trigger times:  9\n",
            "Epoch: 6/25... Step: 1860... Loss: 1.5117... Val Loss: 1.6504\n",
            "trigger times: 0\n",
            "Validation loss 1.650355.  Saving model ...\n",
            "Epoch: 6/25... Step: 1870... Loss: 1.5197... Val Loss: 1.6513\n",
            "trigger times:  1\n",
            "Epoch: 6/25... Step: 1880... Loss: 1.5030... Val Loss: 1.6468\n",
            "trigger times: 0\n",
            "Validation loss 1.646763.  Saving model ...\n",
            "Epoch: 6/25... Step: 1890... Loss: 1.4973... Val Loss: 1.6549\n",
            "trigger times:  1\n",
            "Epoch: 6/25... Step: 1900... Loss: 1.5141... Val Loss: 1.6489\n",
            "trigger times:  2\n",
            "Epoch: 6/25... Step: 1910... Loss: 1.4696... Val Loss: 1.6587\n",
            "trigger times:  3\n",
            "Epoch: 6/25... Step: 1920... Loss: 1.4996... Val Loss: 1.6473\n",
            "trigger times:  4\n",
            "Epoch: 6/25... Step: 1930... Loss: 1.4843... Val Loss: 1.6499\n",
            "trigger times:  5\n",
            "Epoch: 6/25... Step: 1940... Loss: 1.4969... Val Loss: 1.6448\n",
            "trigger times: 0\n",
            "Validation loss 1.644836.  Saving model ...\n",
            "Epoch: 6/25... Step: 1950... Loss: 1.4737... Val Loss: 1.6502\n",
            "trigger times:  1\n",
            "Epoch: 6/25... Step: 1960... Loss: 1.4896... Val Loss: 1.6565\n",
            "trigger times:  2\n",
            "Epoch: 6/25... Step: 1970... Loss: 1.4731... Val Loss: 1.6406\n",
            "trigger times: 0\n",
            "Validation loss 1.640626.  Saving model ...\n",
            "Epoch: 6/25... Step: 1980... Loss: 1.4805... Val Loss: 1.6402\n",
            "trigger times: 0\n",
            "Validation loss 1.640205.  Saving model ...\n",
            "Epoch: 6/25... Step: 1990... Loss: 1.5021... Val Loss: 1.6449\n",
            "trigger times:  1\n",
            "Epoch: 6/25... Step: 2000... Loss: 1.4456... Val Loss: 1.6440\n",
            "trigger times:  2\n",
            "Epoch: 6/25... Step: 2010... Loss: 1.4726... Val Loss: 1.6368\n",
            "trigger times: 0\n",
            "Validation loss 1.636821.  Saving model ...\n",
            "Epoch: 6/25... Step: 2020... Loss: 1.4867... Val Loss: 1.6356\n",
            "trigger times: 0\n",
            "Validation loss 1.635583.  Saving model ...\n",
            "Epoch: 6/25... Step: 2030... Loss: 1.4779... Val Loss: 1.6382\n",
            "trigger times:  1\n",
            "Epoch: 7/25... Step: 2040... Loss: 1.4671... Val Loss: 1.6436\n",
            "trigger times:  2\n",
            "Epoch: 7/25... Step: 2050... Loss: 1.4660... Val Loss: 1.6509\n",
            "trigger times:  3\n",
            "Epoch: 7/25... Step: 2060... Loss: 1.4781... Val Loss: 1.6559\n",
            "trigger times:  4\n",
            "Epoch: 7/25... Step: 2070... Loss: 1.4568... Val Loss: 1.6253\n",
            "trigger times: 0\n",
            "Validation loss 1.625274.  Saving model ...\n",
            "Epoch: 7/25... Step: 2080... Loss: 1.4457... Val Loss: 1.6419\n",
            "trigger times:  1\n",
            "Epoch: 7/25... Step: 2090... Loss: 1.4822... Val Loss: 1.6249\n",
            "trigger times: 0\n",
            "Validation loss 1.624914.  Saving model ...\n",
            "Epoch: 7/25... Step: 2100... Loss: 1.4738... Val Loss: 1.6370\n",
            "trigger times:  1\n",
            "Epoch: 7/25... Step: 2110... Loss: 1.5198... Val Loss: 1.6339\n",
            "trigger times:  2\n",
            "Epoch: 7/25... Step: 2120... Loss: 1.4617... Val Loss: 1.6306\n",
            "trigger times:  3\n",
            "Epoch: 7/25... Step: 2130... Loss: 1.4661... Val Loss: 1.6301\n",
            "trigger times:  4\n",
            "Epoch: 7/25... Step: 2140... Loss: 1.4652... Val Loss: 1.6375\n",
            "trigger times:  5\n",
            "Epoch: 7/25... Step: 2150... Loss: 1.4709... Val Loss: 1.6276\n",
            "trigger times:  6\n",
            "Epoch: 7/25... Step: 2160... Loss: 1.4764... Val Loss: 1.6312\n",
            "trigger times:  7\n",
            "Epoch: 7/25... Step: 2170... Loss: 1.4937... Val Loss: 1.6339\n",
            "trigger times:  8\n",
            "Epoch: 7/25... Step: 2180... Loss: 1.4596... Val Loss: 1.6295\n",
            "trigger times:  9\n",
            "Epoch: 7/25... Step: 2190... Loss: 1.4524... Val Loss: 1.6307\n",
            "trigger times:  10\n",
            "Early stopping! at epoch 6\n"
          ]
        }
      ],
      "source": [
        "train(net=net, train_data=train_data, val_data=val_data, epochs=25, n_seqs=128, n_steps=100, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xI4h6pTqAMwz"
      },
      "outputs": [],
      "source": [
        "def predict(net, char, h=None, cuda=False, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        if cuda:\n",
        "            net.cuda()\n",
        "        else:\n",
        "            net.cpu()\n",
        "        \n",
        "        if h is None:\n",
        "            h = net.init_hidden(1)\n",
        "        \n",
        "        x = np.array([[data.char2int[char]]])\n",
        "        x = data.one_hot_encode(x, len(data.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        if cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = net.forward(inputs, h)\n",
        "\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if cuda:\n",
        "            p = p.cpu()\n",
        "        \n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(data.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "            \n",
        "        return data.int2char[char], h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OUHnlrLWDQa9"
      },
      "outputs": [],
      "source": [
        "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
        "    '''\n",
        "    Generate the next `size` characters from given `prime`\n",
        "    '''\n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()\n",
        "    \n",
        "    # Run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, cuda=cuda, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, cuda=cuda, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_xxTIvdFafL",
        "outputId": "65e157dc-ce7d-4fb3-9cd5-5680609b416d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Juliet to blows,\n",
            "When such a sheeting to the compost to be all\n",
            "this is men on the conclidience; and shall spoke that,\n",
            "And, that I will deay to his head to this with.\n",
            "\n",
            "PETRUCHIO:\n",
            "That thou hast been the corrol of your lady\n",
            "To take their part another strumpets.\n",
            "\n",
            "CASSIUS:\n",
            "What, she was shopen.\n",
            "\n",
            "SICINIUS:\n",
            "I have send me true: and he did see thy learn.\n",
            "That, we another will, a winded to them the down, and\n",
            "with you, sir, the come and see a that would stay.\n",
            "\n",
            "CAPULET:\n",
            "I'll take the temple. The was wants, a she\n"
          ]
        }
      ],
      "source": [
        "print(sample(net, 500, prime='Juliet', top_k=5, cuda=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bak8HE8Fedr",
        "outputId": "c9dd697b-258c-4051-f03e-e78ef0dbffed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Here we have loaded in a model that trained over 1 epoch `rnn_1_epoch.net`\n",
        "with open('rnn.net', 'rb') as f:\n",
        "    state_dict = torch.load(f)\n",
        "    \n",
        "loaded = RNN(input_size=len(data.chars))\n",
        "loaded.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhyNseCqhg7n",
        "outputId": "90e054a7-057a-4bd6-e152-0b3bef5b2ec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Juliet of my like,\n",
            "The words with a traches; but all my house\n",
            "With some mighty and such a man,\n",
            "If stands in tentience to the countery\n",
            "To this me sense of sected sight.\n",
            "\n",
            "MARINA:\n",
            "Why, she hath better stand all as the present serve.\n",
            "\n",
            "Second Gentleman:\n",
            "I have this sorrow strongs thou, shall to stand them:\n",
            "The show with my belessiones, as thou art sent\n",
            "We will be send. He woulds here is there the most the\n",
            "stope, and thanks that humble manys by sufficed with\n",
            "And tell my husband she is not the stain,\n",
            "As it before thee so thou and the some of here,\n",
            "And better that were something to the service.\n",
            "\n",
            "CRESSIDA:\n",
            "What hath me be this time? the man to mildent\n",
            "How much shall help here in mine the chill were,\n",
            "The wantom to her truth, word to secure the course\n",
            "Of the caure is made a man that spiril there and make\n",
            "The change they will the matter of my stang.\n",
            "\n",
            "SUFFOLK:\n",
            "We will not been short on thy both a morn\n",
            "That I am steal. What is myself, make a stain to thy hand,\n",
            "With honest said and sease their brother world\n"
          ]
        }
      ],
      "source": [
        "# Change cuda to True if you are using GPU!\n",
        "print(sample(loaded, 1000, cuda=True, top_k=5, prime=\"Juliet\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BAP_RNN.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
