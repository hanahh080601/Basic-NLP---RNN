{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bap/hana/Basic-NLP-RNN/rnn/rnn\n"
     ]
    }
   ],
   "source": [
    "%cd /home/bap/hana/Basic-NLP-RNN/rnn/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    '''\n",
    "    Config class defines dataset path and hyperparameters.\n",
    "    '''\n",
    "    data_train_url = 'data/shakespeare_train.txt'\n",
    "    data_val_url = 'data/shakespeare_valid.txt'\n",
    "    n_hidden = 512\n",
    "    n_layers = 2\n",
    "    epochs = 25 \n",
    "    n_seqs = 128\n",
    "    n_steps = 100\n",
    "    lr = 0.001\n",
    "    clip = 5\n",
    "    cuda = False\n",
    "    dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    '''\n",
    "    Load data from data path, preprocess (tokenize & one-hot encode) and get data in array type.\n",
    "    '''\n",
    "    def __init__(self, data_train_url = Config.data_train_url, data_val_url = Config.data_val_url):\n",
    "        with io.open (data_train_url, 'r') as f:\n",
    "            self.text_train = f.read()\n",
    "        with io.open (data_val_url, 'r') as f:\n",
    "            self.text_val = f.read()\n",
    "\n",
    "    def char_tokenize(self):\n",
    "        self.chars = tuple(set(self.text_train))\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        self.train_data = np.array([self.char2int[ch] for ch in self.text_train])\n",
    "        self.val_data = np.array([self.char2int[ch] for ch in self.text_val])\n",
    "\n",
    "    def one_hot_encode(self, arr, n_labels):\n",
    "        one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "        one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "        one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "        return one_hot\n",
    "\n",
    "    def get_data(self):\n",
    "        self.char_tokenize()\n",
    "        return self.train_data, self.val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded chars in train: [12 34 36 56 37 40 47 34 37 34 25 38 13 30 49 26 38  4 52 36 38 40 44 38\n",
      " 40 17 36 52  1 38 38  6 40 42 13 51 40  4 60 36 37 63 38 36 61 40 63 38\n",
      " 42 36 40  0 38 40 56 17 38 42 15 58 49 49 53 19 19 30 49 21 17 38 42 15\n",
      " 61 40 56 17 38 42 15 58 49 49 12 34 36 56 37 40 47 34 37 34 25 38 13 30\n",
      " 49 46 52 60]\n",
      "Number of chars in vocab:  67\n",
      "Train text:  First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "data = Dataset()\n",
    "train_data, val_data = data.get_data()\n",
    "print(\"Encoded chars in train:\", train_data[:100])\n",
    "print(\"Number of chars in vocab: \", len(data.chars))\n",
    "print(\"Train text: \", data.text_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    '''\n",
    "    Load data from dataset in batches (batches = n_seqs * n_steps)\n",
    "    '''\n",
    "    def __init__(self, train, val):\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, arr, n_seqs, n_steps):\n",
    "        '''\n",
    "        Create a generator that returns batches of size\n",
    "        n_seqs x n_steps from arr.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        arr: np.array\n",
    "            Array you want to make batches from\n",
    "        n_seqs: int\n",
    "            Batch size, the number of sequences per batch\n",
    "        n_steps: int\n",
    "            Number of sequence steps per batch\n",
    "        '''\n",
    "        batch_size = n_seqs * n_steps\n",
    "        n_batches = len(arr) // batch_size\n",
    "        arr = arr[:n_batches * batch_size]\n",
    "        arr = arr.reshape((n_seqs, -1))\n",
    "        \n",
    "        for n in range(0, arr.shape[1], n_steps):\n",
    "            x = arr[:, n: n + n_steps]\n",
    "            y = np.zeros_like(x)\n",
    "            try:\n",
    "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
    "            except IndexError:\n",
    "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[12, 34, 36, 56, 37]]), array([[34, 36, 56, 37, 40]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = DataLoader(train_data, val_data)\n",
    "next(data_loader(train_data, 1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_steps=Config.n_steps, n_hidden=Config.n_hidden, n_layers=Config.n_layers,\n",
    "                    drop_prob=Config.dropout, lr=Config.lr):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr      \n",
    "        self.lstm = nn.LSTM(vocab_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)        \n",
    "        self.dropout = nn.Dropout(drop_prob)      \n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' \n",
    "        Initialize weights for fully connected layer \n",
    "        '''\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' \n",
    "        Initializes hidden state \n",
    "        '''\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        ''' \n",
    "        Forward pass through the network. \n",
    "        These inputs are x, and the hidden/cell state `hc`. \n",
    "        '''\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = x.reshape(x.size()[0] * x.size()[1], self.n_hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data, val_data, epochs=Config.epochs, n_seqs=Config.n_seqs, \n",
    "          n_steps=Config.n_steps, lr=Config.lr, clip=Config.clip, cuda=Config.cuda):\n",
    "    ''' \n",
    "        Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ----------------\n",
    "        net: RNN network\n",
    "        train_data: text data to train the network\n",
    "        val_data: text data to validate the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        cuda: Train with CUDA on a GPU\n",
    "    '''\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Early stopping\n",
    "    the_last_loss = 100\n",
    "    patience = 10\n",
    "    trigger_times = 0\n",
    "    isStopped = False\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        if isStopped:\n",
    "            break\n",
    "        for x, y in data_loader(train_data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = data.one_hot_encode(x, net.vocab_size)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % 10 == 0:\n",
    "                \n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in data_loader(val_data, n_seqs, n_steps):\n",
    "                    x = data.one_hot_encode(x, net.vocab_size)\n",
    "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "\n",
    "                the_current_loss = np.mean(val_losses)\n",
    "                if the_current_loss > the_last_loss:\n",
    "                    trigger_times += 1\n",
    "                    print('trigger times: ', trigger_times)\n",
    "                    if trigger_times >= patience:\n",
    "                        print('Early stopping! at epoch {0}'.format(e))\n",
    "                        isStopped = True\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    print('trigger times: 0')\n",
    "                    trigger_times = 0\n",
    "                    the_last_loss = the_current_loss\n",
    "                    if not isStopped:\n",
    "                        with open('models/rnn.net', 'wb') as f:\n",
    "                            torch.save({'tokens': data.chars, 'state_dict': net.state_dict()}, f)\n",
    "                        print('Validation loss {:.6f}.  Saving model ...'.format(the_current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(67, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=67, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "net = RNN(vocab_size=len(data.chars))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.4488... Val Loss: 3.4144\n",
      "trigger times: 0\n",
      "Validation loss 3.414395.  Saving model ...\n",
      "Epoch: 1/1... Step: 20... Loss: 3.3175... Val Loss: 3.2808\n",
      "trigger times: 0\n",
      "Validation loss 3.280830.  Saving model ...\n",
      "Epoch: 1/1... Step: 30... Loss: 3.1552... Val Loss: 3.1335\n",
      "trigger times: 0\n",
      "Validation loss 3.133543.  Saving model ...\n",
      "Epoch: 1/1... Step: 40... Loss: 2.9713... Val Loss: 2.9507\n",
      "trigger times: 0\n",
      "Validation loss 2.950736.  Saving model ...\n",
      "Epoch: 1/1... Step: 50... Loss: 2.7752... Val Loss: 2.8395\n",
      "trigger times: 0\n",
      "Validation loss 2.839549.  Saving model ...\n",
      "Epoch: 1/1... Step: 60... Loss: 2.6827... Val Loss: 2.6619\n",
      "trigger times: 0\n",
      "Validation loss 2.661886.  Saving model ...\n",
      "Epoch: 1/1... Step: 70... Loss: 2.5794... Val Loss: 2.5902\n",
      "trigger times: 0\n",
      "Validation loss 2.590239.  Saving model ...\n",
      "Epoch: 1/1... Step: 80... Loss: 2.5564... Val Loss: 2.5312\n",
      "trigger times: 0\n",
      "Validation loss 2.531220.  Saving model ...\n",
      "Epoch: 1/1... Step: 90... Loss: 2.4934... Val Loss: 2.4830\n",
      "trigger times: 0\n",
      "Validation loss 2.482992.  Saving model ...\n",
      "Epoch: 1/1... Step: 100... Loss: 2.4548... Val Loss: 2.4465\n",
      "trigger times: 0\n",
      "Validation loss 2.446502.  Saving model ...\n",
      "Epoch: 1/1... Step: 110... Loss: 2.4043... Val Loss: 2.4175\n",
      "trigger times: 0\n",
      "Validation loss 2.417469.  Saving model ...\n",
      "Epoch: 1/1... Step: 120... Loss: 2.3741... Val Loss: 2.3857\n",
      "trigger times: 0\n",
      "Validation loss 2.385750.  Saving model ...\n",
      "Epoch: 1/1... Step: 130... Loss: 2.3446... Val Loss: 2.3639\n",
      "trigger times: 0\n",
      "Validation loss 2.363907.  Saving model ...\n",
      "Epoch: 1/1... Step: 140... Loss: 2.3194... Val Loss: 2.3446\n",
      "trigger times: 0\n",
      "Validation loss 2.344646.  Saving model ...\n",
      "Epoch: 1/1... Step: 150... Loss: 2.3120... Val Loss: 2.3215\n",
      "trigger times: 0\n",
      "Validation loss 2.321541.  Saving model ...\n",
      "Epoch: 1/1... Step: 160... Loss: 2.2993... Val Loss: 2.3060\n",
      "trigger times: 0\n",
      "Validation loss 2.306039.  Saving model ...\n",
      "Epoch: 1/1... Step: 170... Loss: 2.2964... Val Loss: 2.2854\n",
      "trigger times: 0\n",
      "Validation loss 2.285390.  Saving model ...\n",
      "Epoch: 1/1... Step: 180... Loss: 2.2448... Val Loss: 2.2680\n",
      "trigger times: 0\n",
      "Validation loss 2.267980.  Saving model ...\n",
      "Epoch: 1/1... Step: 190... Loss: 2.2349... Val Loss: 2.2526\n",
      "trigger times: 0\n",
      "Validation loss 2.252625.  Saving model ...\n",
      "Epoch: 1/1... Step: 200... Loss: 2.2123... Val Loss: 2.2359\n",
      "trigger times: 0\n",
      "Validation loss 2.235902.  Saving model ...\n",
      "Epoch: 1/1... Step: 210... Loss: 2.1857... Val Loss: 2.2218\n",
      "trigger times: 0\n",
      "Validation loss 2.221839.  Saving model ...\n",
      "Epoch: 1/1... Step: 220... Loss: 2.2201... Val Loss: 2.2065\n",
      "trigger times: 0\n",
      "Validation loss 2.206481.  Saving model ...\n",
      "Epoch: 1/1... Step: 230... Loss: 2.1704... Val Loss: 2.2007\n",
      "trigger times: 0\n",
      "Validation loss 2.200720.  Saving model ...\n",
      "Epoch: 1/1... Step: 240... Loss: 2.1718... Val Loss: 2.1868\n",
      "trigger times: 0\n",
      "Validation loss 2.186830.  Saving model ...\n",
      "Epoch: 1/1... Step: 250... Loss: 2.1264... Val Loss: 2.1775\n",
      "trigger times: 0\n",
      "Validation loss 2.177464.  Saving model ...\n",
      "Epoch: 1/1... Step: 260... Loss: 2.1192... Val Loss: 2.1626\n",
      "trigger times: 0\n",
      "Validation loss 2.162560.  Saving model ...\n",
      "Epoch: 1/1... Step: 270... Loss: 2.0821... Val Loss: 2.1549\n",
      "trigger times: 0\n",
      "Validation loss 2.154852.  Saving model ...\n",
      "Epoch: 1/1... Step: 280... Loss: 2.1142... Val Loss: 2.1412\n",
      "trigger times: 0\n",
      "Validation loss 2.141172.  Saving model ...\n",
      "Epoch: 1/1... Step: 290... Loss: 2.0748... Val Loss: 2.1267\n",
      "trigger times: 0\n",
      "Validation loss 2.126742.  Saving model ...\n",
      "Epoch: 1/1... Step: 300... Loss: 2.1292... Val Loss: 2.1225\n",
      "trigger times: 0\n",
      "Validation loss 2.122521.  Saving model ...\n",
      "Epoch: 1/1... Step: 310... Loss: 2.0668... Val Loss: 2.1110\n",
      "trigger times: 0\n",
      "Validation loss 2.110972.  Saving model ...\n",
      "Epoch: 1/1... Step: 320... Loss: 2.0767... Val Loss: 2.0966\n",
      "trigger times: 0\n",
      "Validation loss 2.096598.  Saving model ...\n",
      "Epoch: 1/1... Step: 330... Loss: 2.0521... Val Loss: 2.0863\n",
      "trigger times: 0\n",
      "Validation loss 2.086334.  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "train(net=net, train_data=train_data, val_data=val_data, epochs=1, n_seqs=128, n_steps=100, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            net.cuda()\n",
    "        else:\n",
    "            net.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = net.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[data.char2int[char]]])\n",
    "        x = data.one_hot_encode(x, len(data.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = net.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(data.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return data.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "    '''\n",
    "    Generate the next `size` characters from given `prime`\n",
    "    '''\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # Run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juliet that has worther sellen will so hourst an of to this shis firness all the were will he sang thou and the wath stronget the craitings of him a dout al tither at all the theest an meresting morthan thee she his dorstad is sheart, that were as and my head.\n",
      "\n",
      "LARDICHES:\n",
      "What, then' wat ard ale mone the masine, and hin mant me hearte men of in and stor mandes of thou and sentlenstelf me his ant he will be all and have and thou has ar a sore, a done an a truce. \n",
      "And so the cartang to most, wish thou hast, a to may still tither whine me hat her with the canes of thes for his the mean then mist me hear and the stale her to that she seall my hearther hather to be sor and stie hate have wather we troung theng weathing on tees to my lever, and the son the shand then whone stall me this her will him have so with my tood she lead stee to sored, wild my faired thay war the wear hand thank the tous mant have sone, my toush him sto mat here whan a mear man then my lead hows bore merte of a stracters to\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Juliet', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 1 epoch `rnn_1_epoch.net`\n",
    "with open('models/rnn.net', 'rb') as f:\n",
    "    state_dict = torch.load(f, map_location=torch.device('cpu'))\n",
    "    \n",
    "loaded = RNN(vocab_size=len(data.chars))\n",
    "loaded.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juliet to the were a dorting of asting\n",
      "I as the parine mose stink on thin stien the wire.\n",
      "\n",
      "PARIUS:\n",
      "Ay, ar your and and a sond, be then make wer in and mertand of hearter our of hin.\n",
      "\n",
      "PINARO:\n",
      "Then well, and the parcond, than that and the pastays, this.\n",
      "\n",
      "TINO LEONA:\n",
      "I wis my leavens.\n",
      "I he hat an ther ard and that in the wanter, sing is thy serteded, the pearions and sich and by that, this the wert thim all thou would this sing ald\n",
      "Wert it has sto bady then ard that sendes and sell beat\n",
      "Whall homs and maten.\n",
      "\n",
      "SENDOLER:\n",
      "Wire to sor mather that is the the this so my toon the pords, then this the the pantere thise all with hear of all an thing as in atester,\n",
      "And with he be sere the mert alled buld tay the thou standss\n",
      "Whate were to mear hearter,\n",
      "I will sen thou shall the the stant,\n",
      "What when way, that sand and be a somere this were the tone of mowed.\n",
      "\n",
      "LOUTHAMER:\n",
      "And you shat that her, all a men to bear, but harest an well beer to me hin the breat this.\n",
      "\n",
      "PARTIS:\n",
      "How she sere thour als and and and be\n"
     ]
    }
   ],
   "source": [
    "# Change cuda to True if you are using GPU!\n",
    "print(sample(loaded, 1000, cuda=False, top_k=5, prime=\"Juliet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6996669039b24f13aef7be20d9606477de00a70217801a8b6318938f205f3171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
